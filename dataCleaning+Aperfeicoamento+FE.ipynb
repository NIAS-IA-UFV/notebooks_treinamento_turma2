{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-12T02:13:24.53907Z","iopub.execute_input":"2021-12-12T02:13:24.539363Z","iopub.status.idle":"2021-12-12T02:13:25.511584Z","shell.execute_reply.started":"2021-12-12T02:13:24.53933Z","shell.execute_reply":"2021-12-12T02:13:25.510969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Cleaning**\n.\n.\n.\n.\n.","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/titanic/test.csv')\ntrain = pd.read_csv('/kaggle/input/titanic/train.csv')\n\ntrain.describe(include = \"all\")","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:13:25.512733Z","iopub.execute_input":"2021-12-12T02:13:25.513231Z","iopub.status.idle":"2021-12-12T02:13:25.594594Z","shell.execute_reply.started":"2021-12-12T02:13:25.513198Z","shell.execute_reply":"2021-12-12T02:13:25.594006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Análise inicial\n\n#a. Quais as colunas do banco de dados de teste e de treino\nprint(\"As colunas do banco 'Train' são:\\n\")\nprint(train.columns)\n\nprint(\"\\nAs colunas do banco 'Test' são:\\n\")\nprint(test.columns)\n\n#b. Qual o tipo de dado de cada coluna nos dataframes de teste e de treino\nprint(\"\\n\")\ntrain.info()\ntest.info()\n\n#c. Qual a quantidade de valores nulos (NaN) em cada feature\nprint(\"\\n\")\nprint(pd.isnull(train).sum())\nprint(\"\\n\")\nprint(pd.isnull(test).sum())\n\n#d. Realizar um cópia do banco de dados de teste e de treino para que se possa\n# fazer a manipulação sem perder informações\ntrainCopy = train.copy()","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:13:25.595458Z","iopub.execute_input":"2021-12-12T02:13:25.596189Z","iopub.status.idle":"2021-12-12T02:13:25.635553Z","shell.execute_reply.started":"2021-12-12T02:13:25.596156Z","shell.execute_reply":"2021-12-12T02:13:25.632992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Para lidar com valores nulos, podemos preencher estes valores de alguma forma ou\n# descartar a informação. Neste item utilizaremos algumas estratégias para tal.\n\n#a. A feature “Cabin” contém muitos valores nulos e, assim como “Ticket”, não\n# fornece informações relevantes ao modelo à primeira vista. Então pode-se\n# retirá-las.\n\n# (laço de repetição para englobar os dois bancos de dados ao mesmo tempo)\nfor data in [trainCopy, test]:\n    data = data.drop(['Cabin','Ticket'], axis = 1, inplace = True)\n\n#b. A coluna “Age” também contém muitos valores nulos, mas dessa vez iremos\n# preenchê-los. Utilizar a mediana dos valores de idade para isso, é uma boa\n# estratégia inicial.\n\n#c. A feature “Fare” contém poucos valores nulos, e pode ser útil para análises\n# futuras. Pode-se preenchê-la com a mesma estratégia que “Age”, já que\n# ambas são contínuas.\n\n#d. Em “Embarked” também tem-se pouca ocorrência de valores nulos, e como é\n# uma feature categórica, preencher com a moda parece menos impactante na\n# construção do modelo.\nfor data in [trainCopy, test]:\n    data['Age'].fillna(data['Age'].median(), inplace = True)\n    data['Fare'].fillna(data['Fare'].median(), inplace = True)\n    data['Embarked'].fillna(data['Embarked'].mode()[0], inplace = True)\n\n\nprint(trainCopy.isnull().sum())\nprint(\"\\n\")\nprint(test.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:13:25.638462Z","iopub.execute_input":"2021-12-12T02:13:25.639109Z","iopub.status.idle":"2021-12-12T02:13:25.670706Z","shell.execute_reply.started":"2021-12-12T02:13:25.639041Z","shell.execute_reply":"2021-12-12T02:13:25.6674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Para as Features contínuas será útil a criação de grupos para facilitar a análise\n\n#a. Criar Feature que separe a Feature “Age” em 5 intervalos de mesma extensão.\nfor data in [trainCopy,test]:\n    data['Age_group'] = pd.cut(data['Age'].astype(int), 5)\n#b. Criar Feature que separe “Fare” em 6 intervalos que contenham o mesmo número de dados (Não precisam ter a mesma extensão).\n    data['Fare_group'] = pd.qcut(data['Fare'], 6)\n    \ndisplay(trainCopy)\ndisplay(test)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:13:25.672557Z","iopub.execute_input":"2021-12-12T02:13:25.673472Z","iopub.status.idle":"2021-12-12T02:13:25.744999Z","shell.execute_reply.started":"2021-12-12T02:13:25.673425Z","shell.execute_reply":"2021-12-12T02:13:25.744196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Aperfeiçoamento da Modelagem**\n.\n.\n.\n.\n.\n","metadata":{}},{"cell_type":"code","source":"#Será necessário realizar o encoding das variáveis categóricas, no momento, 3\n# tratégias que serão utilizadas são o one-hot, label encoding e ordinal encoding.\n# para fazer isso será necessário relembrar quais são as features categóricas, definir a\n# estratégia que será utilizada, criar as features codificadas e retirar as categóricas\n\ntrainCopy.info()\nprint(\"\\nColunas numéricas:\")\nprint(trainCopy.select_dtypes(include = (\"int64\",\"float64\")))\nprint(\"\\nColunas categóricas:\")\nprint(trainCopy.select_dtypes(include = (\"object\",\"category\")))","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:13:25.746416Z","iopub.execute_input":"2021-12-12T02:13:25.746663Z","iopub.status.idle":"2021-12-12T02:13:25.779088Z","shell.execute_reply.started":"2021-12-12T02:13:25.746625Z","shell.execute_reply":"2021-12-12T02:13:25.778186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Para features sem ordem definida, one-hot encoding pode ser a melhor opção.\nfrom sklearn.preprocessing import OneHotEncoder\n\nonehot = OneHotEncoder(sparse = False)\n\n#Criar uma função que realize o one-hot encode e, como saída, retorna um\n# novo dataframe com as colunas que resultam da codificação, devidamente\n# nomeadas, ao invés das features categóricas:\n\ndef OH_encoder(data,cols):\n    data_encoded = data.copy()\n    for col in cols:      \n        #Criando colunas para One-hot-encode\n        OH_cols = pd.DataFrame(onehot.fit_transform(data[[col]]),dtype = 'int')\n        \n        #Nomeando as colunas\n        OH_cols.columns = onehot.get_feature_names([col])\n        \n        #Adicionando as novas colunas codificadas ao dataframe\n        data_encoded = data_encoded.drop([col], axis = 1)\n        data_encoded = pd.concat([data_encoded,OH_cols], axis = 1)\n    return data_encoded\n\n#Realizando o encoding dos dataframes\ntrain_encoded = OH_encoder(trainCopy,[\"Sex\",\"Embarked\"])\ntest_encoded = OH_encoder(test,[\"Sex\",\"Embarked\"])\n","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:13:25.780357Z","iopub.execute_input":"2021-12-12T02:13:25.780659Z","iopub.status.idle":"2021-12-12T02:13:25.923202Z","shell.execute_reply.started":"2021-12-12T02:13:25.78062Z","shell.execute_reply":"2021-12-12T02:13:25.922351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Para as features que possuem ordem, o label encoder é mais indicado\n# como primeira abordagem.\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel = LabelEncoder()\n\n#Age_group e Fare_group possuem uma sequência, ou seja, possuem ordem\n\n#Realizando Label encoding\nfor data in [train_encoded,test_encoded]:\n    data[\"Age_group_code\"] = label.fit_transform(data[\"Age_group\"])\n    data[\"Fare_group_code\"] = label.fit_transform(data[\"Fare_group\"])\n    data = data.drop([\"Age_group\",\"Fare_group\",\"Name\"], axis = 1, inplace = True)\n\ndisplay(train_encoded)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:13:25.924293Z","iopub.execute_input":"2021-12-12T02:13:25.924497Z","iopub.status.idle":"2021-12-12T02:13:25.960768Z","shell.execute_reply.started":"2021-12-12T02:13:25.924473Z","shell.execute_reply":"2021-12-12T02:13:25.960113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Agora será realizado um novo treinamento e avaliação de modelo, os passos serão os\n# mesmo realizados no item 6.1 g), mas agora com o banco de dados que é resultado\n# do encoding das variáveis categóricas. Após essa avaliação, será possível notar a\n# evolução da precisão do modelo produzido utilizando essas novas técnicas.\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nrandomforest_model1 = RandomForestClassifier(random_state = 0)\n\nX_model1 = train_encoded.drop('Survived', axis = 1)\ny_model1 = train_encoded['Survived']\n\nX_train,X_valid,y_train,y_valid = train_test_split(X_model1,y_model1,test_size = 0.2,random_state = 0)\n\nrandomforest_model1.fit(X_train,y_train)\npreds = randomforest_model1.predict(X_valid)\n\naccuracy = round(accuracy_score(preds,y_valid)*100,2)\nprint(accuracy)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:13:25.961775Z","iopub.execute_input":"2021-12-12T02:13:25.962217Z","iopub.status.idle":"2021-12-12T02:13:26.438008Z","shell.execute_reply.started":"2021-12-12T02:13:25.962176Z","shell.execute_reply":"2021-12-12T02:13:26.437208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Deverá ser criada uma função que produz e avalia pipelines que utilizam diversas\n# estratégias de imputing de valores nulos e encoding de variáveis categóricas.\n\n#a. Colocar de volta os valores nulos das colunas “Age” e “Embarked” do\n# dataframe de treino, utilizando as colunas do banco de dados original.\n\ntrainCopy_nulo = trainCopy.copy()\ntrainCopy_nulo = trainCopy_nulo.drop([\"Age\",\"Embarked\"], axis = 1)\ntrainCopy_nulo = pd.concat([trainCopy_nulo,train[[\"Age\", \"Embarked\"]]], axis = 1)\nprint(trainCopy_nulo.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:13:26.441165Z","iopub.execute_input":"2021-12-12T02:13:26.441487Z","iopub.status.idle":"2021-12-12T02:13:26.45479Z","shell.execute_reply.started":"2021-12-12T02:13:26.441444Z","shell.execute_reply":"2021-12-12T02:13:26.45393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\n#Criar uma função que produza e avalie pipelines que utilizam estratégias indicadas nos argumentos\n\n# data: banco de dados para construção do modelo\n# encoder: estratégia de encoding - one-hot ou label encoder\n# model: algortimo para produzir o modelo, inicialmente random forest classifier\n# numerical_imputer: estratégia para substituir valores nulos numéricos - mean, median\n# categorical_imputer: estratégia para substituir valores nulos categóricos - most frequent\ndef pipeline(data, encoder, model, numerical_imputer = SimpleImputer(),\n                   categorical_imputer = SimpleImputer(strategy = \"most_frequent\")):\n    \n    #Definir as features e o target que serão usados para treinar o modelo e separar o banco de dados utilizando o train-test split.\n    X_pipe = data.drop(\"Survived\", axis = 1)\n    y_pipe = data[\"Survived\"]\n\n    X_train_pipe,X_valid_pipe,y_train_pipe,y_valid_pipe = train_test_split(X_pipe,y_pipe,\n                                                                           test_size = 0.2,\n                                                                           random_state = 0)\n\n    #Criando os passos do preprocessor, para features numéricas e categóricas\n    num_transf = numerical_imputer\n    cat_transf = Pipeline(steps = [(\"imputer\", categorical_imputer),(\"encoder\",encoder)])\n    preprocessor = ColumnTransformer(transformers = [(\"num\",num_transf,[\"Age\"]),\n                                                (\"cat\",cat_transf,[\"Age_group\",\"Fare_group\",\"Sex\",\"Embarked\"])])\n    \n    #Produzir pipeline utilizando o preprocessor previamente construído e o modelo do argumento “model”.\n    #Realizar o treinamento da pipeline, gerar predições e avaliar utilizando accuracy score.\n    pipe = Pipeline(steps = [(\"preprocessor\", preprocessor),(\"model\", model)])\n    pipe.fit(X_train_pipe,y_train_pipe)\n    preds_pipe = pipe.predict(X_valid_pipe)\n\n    #Avaliando as predições\n    accuracy = round(accuracy_score(preds_pipe,y_valid_pipe)*100,2)\n    \n    #A função deve retornar o resultado da avaliação em porcentagem.\n    return accuracy, preds_pipe","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:13:26.456459Z","iopub.execute_input":"2021-12-12T02:13:26.456968Z","iopub.status.idle":"2021-12-12T02:13:26.481195Z","shell.execute_reply.started":"2021-12-12T02:13:26.456925Z","shell.execute_reply":"2021-12-12T02:13:26.480285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Imputers\n#Definir as 4 seguintes estratégias para imputing\nmean_imputer = SimpleImputer()\nmostfreq_imputer = SimpleImputer(strategy = \"most_frequent\")\nmedian_imputer = SimpleImputer(strategy = \"median\")\nzero_imputer = SimpleImputer(strategy = \"constant\", fill_value = 0)\nimputers = [mean_imputer, mostfreq_imputer, median_imputer, zero_imputer]\n\n#Encoders\n#Definir as 2 estratégias para encoding\nfrom sklearn.preprocessing import OrdinalEncoder\nonehot_encoder = OneHotEncoder(sparse = False)\nordinal_encoder = OrdinalEncoder()\nencoders = [onehot_encoder, ordinal_encoder]\n\n#model\nrandomforest = RandomForestClassifier(random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:13:26.484193Z","iopub.execute_input":"2021-12-12T02:13:26.484721Z","iopub.status.idle":"2021-12-12T02:13:26.490922Z","shell.execute_reply.started":"2021-12-12T02:13:26.484687Z","shell.execute_reply":"2021-12-12T02:13:26.490337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#A partir da função de criação de pipelines e das listas de encoders e imputers, criar\n# um loop que produza todas as combinações possíveis dessas estratégias, e mostre o\n# resultado da avaliação de precisão de cada pipeline produzida no loop\n\nfor encoder in encoders:\n    for imputer in imputers:\n        accuracy,_ = pipeline(trainCopy_nulo, encoder, randomforest, numerical_imputer = imputer)\n        print(\"pipe_\"+str(encoder)+'_'+str(imputer)+\" = \"+str(accuracy))\n        \n#precisão inferior àquela encontrada anteriormente. Estratégia de avaliação separada inferior à estratégia mesclada","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:13:26.492019Z","iopub.execute_input":"2021-12-12T02:13:26.49258Z","iopub.status.idle":"2021-12-12T02:13:28.76507Z","shell.execute_reply.started":"2021-12-12T02:13:26.49255Z","shell.execute_reply":"2021-12-12T02:13:28.764243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Cross validation e Gradient boosting**\n.\n.\n.\n.\n.","metadata":{}},{"cell_type":"code","source":"#O cross validation é uma maneira melhor para avaliar os modelos com os quais\n# estamos lidando. Portanto, o modelo que melhor se saiu nos teste até agora, deverá\n# ser avaliado utilizando este método. Como resultado será utilizada a média entre as\n# avaliações\n\nfrom sklearn.model_selection import cross_val_score\n\n#Criando função para gerar avaliação\ndef cross_validation(model, X, y):\n    scores = cross_val_score(model, X, y, cv = 5, scoring = \"accuracy\")\n    return round(scores.mean()*100,2)\n\n#Gerando avaliação do modelo mesclado \naccuracy_crossval = cross_validation(randomforest_model1, X_model1, y_model1)\nprint(accuracy_crossval)\n\n#Assim como o Random Forest, o XGBoost também tem seu equivalente como\n# classifier. O Gradient Boosting Classifier será utilizado agora para gerar um novo\n# modelo, a partir do dataframe que foi manipulado com as melhores estratégias até agora\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n#Criando Função Gradient Boosting classifier para gerar e avaliar um modelo \ndef make_gbc(X,y):\n    gbclass = GradientBoostingClassifier(random_state = 0, n_iter_no_change = 100) \n    gbclass.fit(X,y)\n    score = cross_validation(gbclass, X, y)\n    return score, gbclass\n\nX_model2 = X_model1\ny_model2 = y_model1\n\nscore,gbclass = make_gbc(X_model2,y_model2)\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:13:28.766323Z","iopub.execute_input":"2021-12-12T02:13:28.766631Z","iopub.status.idle":"2021-12-12T02:13:30.884484Z","shell.execute_reply.started":"2021-12-12T02:13:28.7666Z","shell.execute_reply":"2021-12-12T02:13:30.883689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature Engineering**\n.\n.\n.\n.\n.","metadata":{}},{"cell_type":"code","source":"#Os objetivos dessa área são, selecionar as features do banco de dados que sejam\n# menos relacionadas possível entre si, para evitar a redundância, e o mais correlacionadas\n# possível com o target, para que, a partir da variação dessas features, o modelo consiga\n# identificar a variação do target, a fim de predizê-lo.\n\n#O primeiro passo é a avaliação da importância de cada feature que já está no banco\n# de dados de teste, para isso será criada uma função que produza um gráfico indicado\n# o Mutual Information score de cada feature existente no banco de dados d treino, em\n# relação ao target\n\nfrom sklearn.feature_selection import mutual_info_classif\n\n#a. Determinar quais são as features discretas\ndiscrete_features = train_encoded.drop(['Age','Fare'], axis = 1)\n\n#b. Criar a função que produza o gráfico de MI scores\ndef make_MI (X, y, discrete_features):\n    \n    #Produzindo MI scores\n    mi_scores = mutual_info_classif(X, y)\n    mi_scores = pd.Series(mi_scores, index = X.columns)\n    mi_scores = mi_scores.sort_values(ascending = False)\n    \n    #Prduzindo gráfico\n    sns.set_style('whitegrid')\n    scores = mi_scores.sort_values(ascending = True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n    \n    return mi_scores\n\nX = train_encoded.drop(['Survived'],axis = 1)\ny = train_encoded['Survived']\n\nmake_MI(X, y, discrete_features)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:13:30.886194Z","iopub.execute_input":"2021-12-12T02:13:30.886496Z","iopub.status.idle":"2021-12-12T02:13:31.381533Z","shell.execute_reply.started":"2021-12-12T02:13:30.886455Z","shell.execute_reply":"2021-12-12T02:13:31.380697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Após a observação do gráfico gerado é possível concluir que as features que as\n# features com maior influência no target são “Fare”, “Sex” e “Age”, por este motivo, elas\n# serão analisadas mais a fundo por meio da utilização de gráficos. Para facilitar a\n# visualização dos dados, é útil que seja usado um banco de dados que ainda contenha\n# as features categóricas originais\n\n\n#a. Construir dois gráficos do tipo sns.histplot para a análise de “Fare”.\nfig, axs = plt.subplots(1,2, figsize = (18,7))\n\n#Criando gráficos da distribuição de \"Fare\"\naxs[0].set_title(\"Distribuição das Taxas\")\nsns.histplot(x = \"Fare\", kde = True, data = trainCopy, ax = axs[0]);\naxs[1].set_title(\"Relação Entre Taxas e Sobreviventes\")\nsns.histplot(x = \"Fare\",hue = \"Survived\", kde = True, data = trainCopy, ax = axs[1]);\n\n\n#Agora serão criados 4 gráficos também utilizando histogramas com seaborn.\n# Tais gráficos relacionam a Feature “Age” com “Sex”. Para facilitar a\n# visualização é indicada a utilização de plt.subplots, para a visualização de mais\n# de um gráfico por output.\n\nfig, axs = plt.subplots(2,2, figsize = (18,7), sharex = True, sharey = True)\n\naxs[0,0].set_title('Sexo em relação à idade')\nsns.histplot(x = 'Age',hue = 'Sex', kde = True, data = trainCopy, ax = axs[0,0], palette = \"YlOrRd\");\n\naxs[0,1].set_title('Distribuição de sobreviventes por idade, comparando os sexos')\nsns.histplot(data = trainCopy.loc[trainCopy['Survived']==1], x = 'Age', hue = 'Sex', kde = True, ax = axs[0,1], palette = \"YlOrRd_r\");\n\naxs[1,0].set_title('Distribuição de sobrevivência de homens')\nsns.histplot(data = trainCopy.loc[trainCopy['Sex']=='male'], x = 'Age', hue = 'Survived', kde = True, ax = axs[1,0]);\n\naxs[1,1].set_title('Distribuição de sobrevivência de mulheres')\nsns.histplot(data = trainCopy.loc[trainCopy['Sex']=='female'], x = 'Age', hue = 'Survived', kde = True, ax = axs[1,1]);","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:13:31.382974Z","iopub.execute_input":"2021-12-12T02:13:31.383283Z","iopub.status.idle":"2021-12-12T02:13:34.478309Z","shell.execute_reply.started":"2021-12-12T02:13:31.383242Z","shell.execute_reply":"2021-12-12T02:13:34.477451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature Construction**\n.\n.\n.\n.\n.","metadata":{}},{"cell_type":"code","source":"#Este passo do feature engineering vai para o lado oposto em relação ao feature\n# selection, já que aumenta o número de features. Em uma interpretação espacial, podemos\n# dizer que cada feature do banco de dados é uma dimensão, feature construction aumenta\n# este número de eixos do espaço do banco de dados, enquanto feature selection diminui.\n\n#a. A partir de “Name” criar a feature “Title” para separar os valores,\n# primeiramente, pela vírgula, e depois pelo ponto. Para isso deve-se utilizar o\n# método str.split, lembre-se de criar uma nova coluna para armazenar estas\n# informações, após isso, pode-se excluir a feature “Names”\n\n# Adicionando \"Name\" à train_encoded\ntrain_encoded = pd.concat([train_encoded,trainCopy[\"Name\"]], axis = 1)\n\n#Utilizando o método string split do pandas para retirar informações de texto \nfor data in [train_encoded,test]:\n    data[\"Title\"] = data[\"Name\"].str.split(',',expand = True)[1].str.split('.',expand = True)[0]\n    data.drop(\"Name\", axis = 1, inplace = True)\n\n#Demonstrando quais os valores da coluna\nprint(train_encoded[\"Title\"].value_counts())\n\n\n#Criar novas Features para resumir informações de outras já existentes, uma para\n# indicar o número total de familiares de cada passageiro, sejam eles primos, filhos ou\n# acompanhantes, e outra para informar se o passageiro está sozinho ou não.\n\n#a. Criar feature que indique o tamanho da família, somando “SibSp” e “Parch”\nfor data in [train_encoded,test]:\n    data[\"Família\"] = data[\"SibSp\"] + data[\"Parch\"]\n    \n#b. Criar feature booleana indicando se o passageiro está sozinho, sendo,\n# verdadeiro caso não tenha nenhum familiar a bordo, e falso caso tenha.\n    data[\"Sozinho\"] = 1\n    for i in range(len(data.index)):\n        if data.loc[i,\"Família\"] != 0:\n            data.loc[i,\"Sozinho\"] = 0\n\ntrain_encoded.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:13:34.479829Z","iopub.execute_input":"2021-12-12T02:13:34.480273Z","iopub.status.idle":"2021-12-12T02:13:34.684372Z","shell.execute_reply.started":"2021-12-12T02:13:34.480233Z","shell.execute_reply":"2021-12-12T02:13:34.68357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature Extraction**\n.\n.\n.\n.\n.","metadata":{}},{"cell_type":"code","source":"#1. No banco de dados em que estamos trabalhando existem duas features com valores\n# contínuos, são \"Fare'' e “Age”, portanto, a partir da interação entre elas pode-se criar\n# grupos utilizando K-means como forma de clustering\n\n\n#Visualizando a interação entre \"Fare\" e \"Age\"\nsns.scatterplot(x=\"Fare\",y=\"Age\",data = trainCopy);\n\nfrom sklearn.cluster import KMeans\n\n#outliers - dados extremamentes fora do padrão\n#Criando dataframe com \"Age\" e \"Fare\" sem outliers\nfare_age_group = trainCopy.loc[trainCopy[\"Fare\"] < 500,[\"Fare\",\"Age\"]]\n\n#Criando feature a partir da interação entre \"Age\" e \"Fare\"\nkmeans = KMeans(n_clusters = 6, random_state = 0)\nfare_age_group[\"Fare_age_group\"] = kmeans.fit_predict(fare_age_group)\nfare_age_group[\"Fare_age_group\"] = fare_age_group[\"Fare_age_group\"].astype(\"int\")\n\nfare_age_group\n","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:17:49.393326Z","iopub.execute_input":"2021-12-12T02:17:49.393661Z","iopub.status.idle":"2021-12-12T02:17:49.987444Z","shell.execute_reply.started":"2021-12-12T02:17:49.393631Z","shell.execute_reply":"2021-12-12T02:17:49.986595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#c. Agora será útil entender quais as características de cada grupo criado pelo\n# algoritmo, para isso serão utilizados dois gráficos, um para identificar os\n# grupos na distribuição “Age” x “Fare”, outro para avaliá-los em relação à\n# chance de sobrevivência.\n\n#Adicionando feature criada ao banco de dados de treino\ntrainCopy['Fare_age_group'] = fare_age_group['Fare_age_group']\n\n#Criando visualização para entendimento da nova feature\nfig, axs = plt.subplots(1,2, figsize = (20,7))\nsns.barplot(x = 'Fare_age_group', y = 'Survived', data = trainCopy, ax = axs[1], palette = \"rainbow_r\")\nsns.scatterplot(x='Fare',y='Age', hue = 'Fare_age_group', data = trainCopy, ax = axs[0],palette = \"rainbow_r\" );","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:21:08.53729Z","iopub.execute_input":"2021-12-12T02:21:08.537633Z","iopub.status.idle":"2021-12-12T02:21:09.54935Z","shell.execute_reply.started":"2021-12-12T02:21:08.537588Z","shell.execute_reply":"2021-12-12T02:21:09.548383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ao analisarmos a nova feature, é fácil notar que existem valores nulos, estes valores\n# correspondem aos outliers que foram retirados para a construção dos grupos. Para\n# substituir os NaN, deve-se identificar quem são esses outliers e designar,\n# manualmente, seu grupo\n\n#Descobrindo outliers\ntrainCopy.loc[trainCopy[\"Fare_age_group\"].isnull() == True,:]\n\n#Selecionando um grupo para os Outliers\ntrainCopy.loc[trainCopy[\"Fare_age_group\"].isnull() == True,[\"Fare_age_group\"]] = 1\ntrainCopy.loc[[258,679,737]]\n\n#Adicionando a nova feature aos banco de dado train_encoded\ntrain_encoded[\"Fare_age_group\"] = trainCopy[\"Fare_age_group\"]\ntrain_encoded","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:33:03.963165Z","iopub.execute_input":"2021-12-12T02:33:03.963898Z","iopub.status.idle":"2021-12-12T02:33:03.999235Z","shell.execute_reply.started":"2021-12-12T02:33:03.963855Z","shell.execute_reply":"2021-12-12T02:33:03.998295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#As features originais não representam a mesma grandeza, “Age” conta a idade em\n# anos e “Fare” a taxa de embarque em dólares, portanto, para que seja possível utilizar\n# o PCA, deve-se primeiramente realizar os scaling das variáveis. para esta etapa será\n# utilizado o MinMax Scaler do scikit learn\n\n\nfrom scipy import stats\nfrom sklearn.preprocessing import MinMaxScaler\n\n#Criando scaller \nscaller = MinMaxScaler(copy = False)\n\n#Criando dataframe para gravar as features após realizar scalling\nstand_data = pd.DataFrame()\n\n#Loop para a realização do scalling\nfor column in [\"Fare\",\"Age\"]:\n    stand_data[[column]] = trainCopy[[column]]\n    stand_data[[column]] = scaller.fit_transform(stand_data[[column]])\n    \n#Gerando gráficos para mostrar a distribuição das features após o scalling \nfig, axs = plt.subplots(1,2, figsize = (18,7))\n\nsns.histplot(x = \"Age\", kde = True, data = stand_data, ax = axs[0]);\nsns.histplot(x = \"Fare\", kde = True, data = stand_data, ax = axs[1]);","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:38:26.538033Z","iopub.execute_input":"2021-12-12T02:38:26.538453Z","iopub.status.idle":"2021-12-12T02:38:27.362492Z","shell.execute_reply.started":"2021-12-12T02:38:26.538423Z","shell.execute_reply":"2021-12-12T02:38:27.36155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Agora já é possível realizar o PCA, contudo, não há garantias que esta estratégia trará\n# novas informações úteis para o modelo, portanto, é necessário, após a criação dos\n# novos eixos, analisar criticamente se vale a pena introduzi-los ao banco de dados.\n\nfrom sklearn.decomposition import PCA\n\n#Realizando o PCA de \"Fare\" e \"Age\" \npca = PCA()\npca_data = pca.fit_transform(stand_data)\n\n#Criando um dataframe para armazenar as features criadas a partir do PCA\ncolumns = ['PC1','PC2']\npca_data = pd.DataFrame(pca_data, columns = columns)\n\n#Criando dataframe para identificar como os eixos foram afetados pelo PCA\nloadings = pd.DataFrame(pca.components_.T, columns = columns, index = [\"Fare\",\"Age\"])\nprint(loadings)\nprint(\"\\n\")\nprint(pca_data)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:45:58.599931Z","iopub.execute_input":"2021-12-12T02:45:58.600485Z","iopub.status.idle":"2021-12-12T02:45:58.616812Z","shell.execute_reply.started":"2021-12-12T02:45:58.600444Z","shell.execute_reply":"2021-12-12T02:45:58.616115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(sem interação entre os eixos: PC1 ~ Age e PC2 ~ Fare)","metadata":{}},{"cell_type":"code","source":"#Gerando MI scores das features PC1 e PC2\nmake_MI(pca_data, trainCopy[\"Survived\"], discrete_features = False)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:41:08.696155Z","iopub.execute_input":"2021-12-12T02:41:08.696442Z","iopub.status.idle":"2021-12-12T02:41:08.939135Z","shell.execute_reply.started":"2021-12-12T02:41:08.69641Z","shell.execute_reply":"2021-12-12T02:41:08.938464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Score MI muito baixos, não se mostrando válido adiconar ao banco de testes","metadata":{}},{"cell_type":"markdown","source":"**Target Encoding**\n.\n.\n.\n.\n.","metadata":{}},{"cell_type":"code","source":"#Até agora, quase todas as features categóricas já foram codificadas, apenas uma\n# ainda resta. No item 9.2 foi criada a feature “Title”, a partir de “Name”, essa coluna contém\n# diversos valores únicos, de forma que é promissora a utilização do target encoding para\n# codificá-la. Esta estratégia utiliza a relação da feature categórica com o target para substituir\n# seus valores, ou seja, é uma estratégia considerada de supervised learning. Para o target\n# encoding será utilizado o algoritmo MEstimateEncoder\n\n# Deve-se então realizar o encoding da feature “Title”, após isso será interessante\n# analisar a distribuição dos valores resultantes e compará-los com o target\n\n#a. Encoding da feature “Title”\nfrom category_encoders import MEstimateEncoder\n\nencoder = MEstimateEncoder(cols = [\"Title\"], m = 5)\n\ntitle = encoder.fit_transform(train_encoded[\"Title\"], train_encoded[\"Survived\"])\nprint(title)\n\n#b. Plot da distribuição dos valores codificados\nfig, ax = plt.subplots(figsize = (20,9))\n\nsns.distplot(train_encoded[\"Survived\"], kde=False, norm_hist=True)\nsns.kdeplot(title[\"Title\"], color='r', ax=ax)\nax.set_xlabel(\"Survived\")\nax.legend(labels=[\"Title\", \"Survived\"]);\n\ntrain_encoded[\"Title-encoded\"] = title\ntrain_encoded.info()\n","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:54:45.324673Z","iopub.execute_input":"2021-12-12T02:54:45.324959Z","iopub.status.idle":"2021-12-12T02:54:45.711837Z","shell.execute_reply.started":"2021-12-12T02:54:45.32493Z","shell.execute_reply":"2021-12-12T02:54:45.710693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_enc = train_encoded.loc[:,['Fare_age_group','Title-encoded']]\ny_enc = train_encoded['Survived']\nmake_MI(X_enc, y_enc, discrete_features = False )","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:54:40.394296Z","iopub.execute_input":"2021-12-12T02:54:40.394617Z","iopub.status.idle":"2021-12-12T02:54:40.646805Z","shell.execute_reply.started":"2021-12-12T02:54:40.394581Z","shell.execute_reply":"2021-12-12T02:54:40.645746Z"},"trusted":true},"execution_count":null,"outputs":[]}]}