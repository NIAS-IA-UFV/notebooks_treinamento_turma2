{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use(\"seaborn-whitegrid\")\n\nnp.random.seed(0)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-14T02:21:25.857869Z","iopub.execute_input":"2021-12-14T02:21:25.858167Z","iopub.status.idle":"2021-12-14T02:21:25.874361Z","shell.execute_reply.started":"2021-12-14T02:21:25.858135Z","shell.execute_reply":"2021-12-14T02:21:25.87337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#a. Importar o banco de dados utilizando o pandas\nbancoTempo = pd.read_csv(\"/kaggle/input/szeged-weather/weatherHistory.csv\")\ndisplay(bancoTempo)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T02:21:25.876017Z","iopub.execute_input":"2021-12-14T02:21:25.876916Z","iopub.status.idle":"2021-12-14T02:21:26.212542Z","shell.execute_reply.started":"2021-12-14T02:21:25.876875Z","shell.execute_reply":"2021-12-14T02:21:26.211427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#b. Identificar o número de valores faltantes em cada colunas\nbancoTempo.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-14T02:21:26.214536Z","iopub.execute_input":"2021-12-14T02:21:26.214818Z","iopub.status.idle":"2021-12-14T02:21:26.263226Z","shell.execute_reply.started":"2021-12-14T02:21:26.214784Z","shell.execute_reply":"2021-12-14T02:21:26.26227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Coluna \"Precip Type\" retornando 517 ocorrências vazias, podendo significar que nesses respectivos dias não houve nenhum tipo de precipitação","metadata":{}},{"cell_type":"code","source":"#c. Gerar momentos estatísticos do banco de dados\nbancoTempo.describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-14T02:21:26.406674Z","iopub.execute_input":"2021-12-14T02:21:26.407155Z","iopub.status.idle":"2021-12-14T02:21:26.474168Z","shell.execute_reply.started":"2021-12-14T02:21:26.407094Z","shell.execute_reply":"2021-12-14T02:21:26.473107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Coluna \"Loud Cover\" saindo do padrão do banco, uma vez que está retornando apenas valores '0'","metadata":{}},{"cell_type":"code","source":"#d. Utilizando o método “.hist” do pandas realize plots de distribuição dos valores das colunas\nbancoTempo[\"Pressure (millibars)\"].hist()\nprint(bancoTempo.loc[bancoTempo[\"Pressure (millibars)\"] == 0, [\"Pressure (millibars)\"]].value_counts())\nprint(\"\\n\")\ncolunasNumericas = bancoTempo.select_dtypes(include = [\"int64\",\"float64\"]).keys()\nprint(\"Colunas numéricas\")\nprint(colunasNumericas)\nbancoTempo[colunasNumericas].hist(figsize = (18,12))","metadata":{"execution":{"iopub.status.busy":"2021-12-14T02:21:26.476358Z","iopub.execute_input":"2021-12-14T02:21:26.47695Z","iopub.status.idle":"2021-12-14T02:21:28.601382Z","shell.execute_reply.started":"2021-12-14T02:21:26.476905Z","shell.execute_reply":"2021-12-14T02:21:28.60022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Coluna \"Pressure (millibars)\" retornando ocorrências iguais a '0'","metadata":{}},{"cell_type":"code","source":"#Deve-se agora definir quais são as features categóricas e quais são as numéricas,\n# para isso existe o método “.select_dtypes”, aqui será necessário identificar apenas os\n# nomes das features\n\nbancoTempo.info()\n\ncolunasCategoricas = bancoTempo.select_dtypes(include = [\"object\"]).keys()\nprint(\"\\nColunas Categóricas\")\nprint(colunasCategoricas)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T02:21:28.602936Z","iopub.execute_input":"2021-12-14T02:21:28.603432Z","iopub.status.idle":"2021-12-14T02:21:28.662509Z","shell.execute_reply.started":"2021-12-14T02:21:28.603399Z","shell.execute_reply":"2021-12-14T02:21:28.661535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"Formatted Date\" está como object, mas não é categórica","metadata":{}},{"cell_type":"code","source":"#Inspecionando os valores únicos de cada feature categórica\nprint(bancoTempo[\"Precip Type\"].value_counts())\nprint(\"\")\nprint(bancoTempo[\"Daily Summary\"].value_counts())\nprint(\"\")\nprint(bancoTempo[\"Summary\"].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-12-14T02:21:28.665067Z","iopub.execute_input":"2021-12-14T02:21:28.665457Z","iopub.status.idle":"2021-12-14T02:21:28.710555Z","shell.execute_reply.started":"2021-12-14T02:21:28.665418Z","shell.execute_reply":"2021-12-14T02:21:28.709311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Os 3 últimos valores de \"Summary\" ocorrem apenas 1 vez cada, sendo necessário retirá-los","metadata":{}},{"cell_type":"code","source":"#A partir das informações obtidas durante a exploração do banco de dados será agora\n# necessário realizar sua limpeza. Aqui serão usadas pipelines que, após configuradas,\n# realizarão a substituição dos valores faltantes, encoding das variáveis categóricas e, como\n# saída, gerarão um objeto do tipo np.array, necessário para o treino da rede neural.\n\n# Retirando a indicação de fuso-horário\nbancoTempo[\"Date\"] = bancoTempo[\"Formatted Date\"].str.split(' ',expand = True)[0] + ' ' + bancoTempo[\"Formatted Date\"].str.split(' ',expand = True)[1]\nbancoTempo[\"Date\"] = pd.to_datetime(bancoTempo[\"Date\"])\nbancoTempo.drop(\"Formatted Date\", axis = 1, inplace = True)\n\n# Adicionando colunas de hora, dia, mês e ano\nbancoTempo[\"Year\"] = bancoTempo[\"Date\"].dt.year\nbancoTempo[\"Month\"] = bancoTempo[\"Date\"].dt.month\nbancoTempo[\"Day\"] = bancoTempo[\"Date\"].dt.day\nbancoTempo[\"Hour\"] = bancoTempo[\"Date\"].dt.hour\nbancoTempo.drop(\"Date\", axis = 1, inplace = True)\nbancoTempo.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-14T02:21:28.711999Z","iopub.execute_input":"2021-12-14T02:21:28.712263Z","iopub.status.idle":"2021-12-14T02:21:29.752875Z","shell.execute_reply.started":"2021-12-14T02:21:28.712231Z","shell.execute_reply":"2021-12-14T02:21:29.751778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Agora será criada a pipeline para completar a limpeza dos dados, com ela será\n# realizada a substituição dos valores faltantes nas colunas “Pressure (millibars)” e\n# “Precip Type” e o encoding, utilizando ordinal encoder, das variáveis categóricas. Para\n# isso será utilizada a biblioteca Column Transformer do sklearn, mas antes deverão ser\n# retirados os dados que não servirão para o treinamento do modelo.\n\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder\n\n#Retirando a data e o target das colunas categóricas\ncolunasCategoricas = colunasCategoricas.drop([\"Formatted Date\", \"Daily Summary\"])\n\n#Retirando colunas e linhas desnecessárias\nbancoTempo.drop(\"Loud Cover\", axis = 1, inplace = True)\nbancoTempo.drop(bancoTempo.loc[bancoTempo[\"Summary\"].isin([\"Breezy and Dry\",\"Windy and Dry\",\"Dangerously Windy and Partly Cloudy\"])].index, inplace=True)\n\n#Criando imputers e encoder\nimputer = SimpleImputer(missing_values=0)\ncat_transf = Pipeline(steps=[(\"imputer\",SimpleImputer(strategy = \"constant\", fill_value = \"none\")),\n                             (\"encoder\",OrdinalEncoder())]) \n\n#Criando pipeline para realizar as trasnformações nas colunas\ntransformer = ColumnTransformer(transformers=[(\"press\", imputer, [\"Pressure (millibars)\"]),\n                                              (\"categorical\", cat_transf, colunasCategoricas)], remainder = \"passthrough\")","metadata":{"execution":{"iopub.status.busy":"2021-12-14T02:21:29.754713Z","iopub.execute_input":"2021-12-14T02:21:29.755039Z","iopub.status.idle":"2021-12-14T02:21:29.797002Z","shell.execute_reply.started":"2021-12-14T02:21:29.755006Z","shell.execute_reply":"2021-12-14T02:21:29.796086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#O último passo da limpeza dos dados será a separação em treino e validação,\n# aplicação da pipeline de limpeza e o encoding da coluna que será o target (“Daily summary”)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Realizando a separação do banco de dados\nX = bancoTempo.drop(\"Daily Summary\", axis = 1)\ny = bancoTempo[\"Daily Summary\"]\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, stratify = y)\n\n# Aplicando pipeline\nX_train = transformer.fit_transform(X_train)\nX_test = transformer.transform(X_test)\n\n# Realizando o encoding do target\nlabel = LabelEncoder()\ny_train = label.fit_transform(y_train)\ny_test = label.transform(y_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T02:21:29.798275Z","iopub.execute_input":"2021-12-14T02:21:29.79914Z","iopub.status.idle":"2021-12-14T02:21:30.134521Z","shell.execute_reply.started":"2021-12-14T02:21:29.799103Z","shell.execute_reply":"2021-12-14T02:21:30.133312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Primeiramente deve-se criar a arquitetura da rede neural, ela terá duas camadas\n# escondidas, sendo cada uma com uma saída de dimensão 256, entre elas serão\n# utilizados Batch normalization e Dropout. Como funções de ativação serão usados\n# “relu” para as camadas escondidas e o “softmax” na camada de saída, esta função\n# serve para que o modelo realize predições na forma de classificação não binária, ou\n# seja, que seja capaz de prever mais de duas classes.\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n#Identificando dimensões de entrada e saída\ninput_size = [X.shape[1]]\noutput_size = y.unique().shape[0]\n\n#Criando arquietura do modelo\nmodel = keras.Sequential([layers.BatchNormalization(input_shape = input_size),\n                          layers.Dense(256,activation = \"relu\"),\n                          layers.BatchNormalization(),\n                          layers.Dropout(0.3),\n                          layers.Dense(256,activation = \"relu\"),\n                          layers.BatchNormalization(),\n                          layers.Dropout(0.3),\n                          layers.Dense(output_size,activation = \"softmax\")])\n\n#Realizando o tratamento do target\ny_train = keras.utils.to_categorical(y_train)\ny_test = keras.utils.to_categorical(y_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T02:21:30.135894Z","iopub.execute_input":"2021-12-14T02:21:30.136144Z","iopub.status.idle":"2021-12-14T02:21:30.287364Z","shell.execute_reply.started":"2021-12-14T02:21:30.13611Z","shell.execute_reply":"2021-12-14T02:21:30.286211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Por fim, será realizado o treinamento do modelo e plotagem dos resultados das\n# avaliações, será utilizado o early stopping para evitar o overfitting.\n\nmodel.compile(optimizer = \"adam\",\n             loss = \"categorical_crossentropy\",\n             metrics = [\"categorical_accuracy\"])\n\n#a. Configurar early stopping - Utilizar a biblioteca do keras para configurar early stopping, com uma\n# tolerância de 10 épocas e uma variação mínima no resultado da função\n# de perda de 0.001\nearly_stopping = keras.callbacks.EarlyStopping(patience = 10,\n                                               min_delta = 0.001,\n                                               restore_best_weights = True)\n\n#b. Realizar o treino e armazenar os resultados da avaliação do modelo\nhistory = model.fit(X_train, y_train,\n                    validation_data = (X_test, y_test),\n                    batch_size = 3000,\n                    epochs = 300,\n                    callbacks = [early_stopping])\n\n#c. Para a plotagem deve-se transformar os resultados do treino em um dataframe\n# do pandas e plotar os valores de perda e acurácia em função das épocas\n\nhistory = pd.DataFrame(history.history)\nfig, axs = plt.subplots(1,2, figsize = (18,6))\n\naxs[0].set_title(\"Cross-Entropy\")\nsns.lineplot(data = history.loc[:,[\"loss\", \"val_loss\"]], ax = axs[0])\n\naxs[1].set_title(\"Accuracy\")\nsns.lineplot(data = history.loc[:,[\"categorical_accuracy\", \"val_categorical_accuracy\"]], ax = axs[1])","metadata":{"execution":{"iopub.status.busy":"2021-12-14T02:21:30.290164Z","iopub.execute_input":"2021-12-14T02:21:30.290427Z","iopub.status.idle":"2021-12-14T02:29:27.601616Z","shell.execute_reply.started":"2021-12-14T02:21:30.290395Z","shell.execute_reply":"2021-12-14T02:29:27.600849Z"},"trusted":true},"execution_count":null,"outputs":[]}]}